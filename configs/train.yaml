# configurations for training
task_prefix: HYBRID_TRAIN
batch_size: 8  # batch size on single device
epochs: 500
target_pred: v
checkpoint_interval: 2  # save checkpoint every n epochs
seed: 42
deterministic: False
logging_dir: "./logs"  # better absolute path, to revise

motif_conditioning: False
moe_conditioning: False
self_conditioning: False

resume:
  ckpt_dir: null
  ema_dir: null
  load_model_only: True

ema:
  decay: 0.999  # EMA decay rate
  mutable_param_keywords: [""]

noise:
  mode: mix_up02_beta
  p1: 1.9
  p2: 1.0

loss:
  moe_loss_weight: 0.3  # empirically chosen

data:
  data_dir: ./data/hybrid_train/
  plm_emb_dir: ./data/hybrid_train/embedding/
  complex_dir: ./data/hybrid_train/complex_contacts.csv
  complex_prop: 0.8
  crop_size: 256  # number of residues per crop
  format: "pdb"
  overwrite: False
  batch_padding: True
  sampling_mode: "cluster-random"
  num_workers: 6 # Number of workers for dataloader
  pin_memory: True # Pin memory for dataloader
  fraction: 1.0
  molecule_type: null
  experiment_types: null 
  min_length: null
  max_length: 256
  oligomeric_min: null
  oligomeric_max: null
  best_resolution: null
  worst_resolution: null
  train_val_prop: [0.99, 0.01]
  split_type: "sequence_similarity"
  split_sequence_similarity: 0.9
  overwrite_sequence_clusters: False

model:
  training: True
  token_dim: 768  # dimension of the tokens in the sequence
  nlayers: 10  # number of transformer layers
  nheads: 12  # number of attn heads
  residual_mha: True  # whether to use a residual connection in the mha
  residual_transition: True  # whether to use a residual connection in the transition
  parallel_mha_transition: False  # whether to compute mha and transition as parallel and add them up (AF3 style) or sequentially (normal transofrmers)
  use_attn_pair_bias: True  # whether to bias attention using a bias coming from a pair representation

  strict_feats: False  # if False, then fills missing features with default values (e.g. chain break with zero, residue sequence index by [0, 1, 2, ...], etc)
  # If True, if some feature is not provided, then it raises an error

  feats_init_seq: ["plm_emb", "res_type", "res_idx", "chain_break_per_res"]  # Sequence features to include in initial representation
  feats_cond_seq: ["time_emb"]  # Sequence features to include in conditioning variables

  # Parameters for the features we extract (both for sequence representatoin and conditioning vector)
  t_emb_dim: 256  # dimension of the time embedding
  idx_emb_dim: 128  # dimension of the residue index embedding
  dim_cond: 512  # dimension of conditioning vector
  plm_in_dim: 1280
  plm_out_dim: 256

  feats_pair_repr: ["xt_pair_dists", "rel_pos"]  # Features to include in the pair representation
  feats_pair_cond: ["time_emb"]  # Features to include in the pair representation conditioning

  # Parameters for the pair features we extract
  # Binning for the pair distances of noisy xt
  xt_pair_dist_dim: 64
  xt_pair_dist_min: 0.1  # in nm (not ?)
  xt_pair_dist_max: 3  # in nm (not ?)
  r_max: 32  # maximum relative position (in sequence index) to consider

  # Dimension of final pair representation
  pair_repr_dim: 512
  num_registers: 10
  use_qkln: True

  use_moe: True
  n_experts: 5
  n_activated_experts: 2
  dim_moe_cond: 0
  capacity_factor: 1.3
  normalize_expert_weights: True

optimizer:
  lr: 0.0001
  weight_decay: 0.
  beta1: 0.9
  beta2: 0.999
  use_adamw: False
  lr_scheduler: "af3"
  warmup_steps: 4000
  decay_every_n_steps: 80000
  decay_factor: 0.98

defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  run:
    dir: .
  output_subdir: null

