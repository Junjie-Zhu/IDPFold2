_target_: src.models.idpfold_multimer.IDPFoldMultimer

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 1e-4
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

net:
  _target_: src.models.components.diffusion_module.DiffusionModule
  'training': True
  'embedding_channel': 1280
  'token_channel': 256
  'diffusion_token_channel': 384
  'embedding_module':
    _target_: src.models.components.embedder.EmbeddingModule
    init_embed_size: 256
    node_embed_size: 256
    edge_embed_size: 128
    num_bins: 22
    min_bin: 1e-5
    max_bin: 20.0
    self_conditioning: true
# 'diffusion_conditioning':
#     _target_: src.models.components.diffusion_module.DiffusionConditioning
#     'token_channel': 384
#     'token_pair_channel': 128
  'atom_attention_encoder':
    _target_: src.models.components.diffusion_module.AtomAttentionEncoder
    'in_token_channel': 256  # the same as token_channel
    'out_token_channel': 384  # the same as diffusion_token_channel
    'token_pair_channel': 128
    'atom_channel': 128
    'atom_pair_channel': 16
    'use_dense_mode': True
    'atom_transformer':
      _target_: src.models.components.diffusion_module.AtomTransformer
      'n_query': 32
      'n_key': 128
      'diffusion_transformer':
        _target_: src.models.components.diffusion_module.DiffusionTransformer
        'a_channel': 128  # the same as atom_channel
        's_channel': 128  # the same as atom_channel
        'z_channel': 16  # the same as atom_pair_channel
        'n_block': 3
        'n_head': 4
  'diffusion_transformer_main':
    _target_: src.models.components.diffusion_module.DiffusionTransformer
    'a_channel': 384  # the same as diffusion_token_channel
    's_channel': 256  # the same as token_channel
    'z_channel': 128  # the same as token_pair_channel
    'n_block': 8
    'n_head': 16
  'atom_attention_decoder':
    _target_: src.models.components.diffusion_module.AtomAttentionDecoder
    'in_token_channel': 384  # the same as diffusion_token_channel
    'atom_channel': 128
    'final_zero_init': False,
    'atom_transformer':  # the same as that in atom_attention_encoder
      _target_: src.models.components.diffusion_module.AtomTransformer
      'n_query': 32
      'n_key': 128
      'diffusion_transformer':
        _target_: src.models.components.diffusion_module.DiffusionTransformer
        'a_channel': 128  # the same as atom_channel
        's_channel': 128  # the same as atom_channel
        'z_channel': 16  # the same as atom_pair_channel
        'n_block': 3
        'n_head': 4

# default hparam for sampling s2011
inference:
  n_replica: 5
  replica_per_batch: 5
  self_conditioning: true
  output_dir: ${paths.output_dir}/samples

# compile model for faster training with pytorch 2.0
compile: false
