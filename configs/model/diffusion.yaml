_target_: src.models.idpfold_multimer.IDPFoldMultimer

net:
  _target_: src.models.components.diffusion_module.DiffusionModule
  use_fine_grained_checkpoint: False
  sigma_data: 16.
  c_token: 384
  c_atom: 128
  c_atompair: 16
  c_z: 128
  c_s: 384
  c_s_inputs: 1280
  initialization:
    zero_init_condition_transition: False
    zero_init_atom_encoder_residual_linear: False
    he_normal_init_atom_encoder_small_mlp: False
    he_normal_init_atom_encoder_output: False
    glorot_init_self_attention: False
    zero_init_adaln: True
    zero_init_residual_condition_transition: False
    zero_init_dit_output: True
    zero_init_atom_decoder_linear: False
  embedding_module:
    _target_: src.models.components.diffusion_module.EmbeddingModule
    init_embed_size: 256
    node_embed_size: 384
    edge_embed_size: 128
    embedding_size: 1280
    sigma_data: 16.0
    num_bins: 22
    min_bin: 1e-5
    max_bin: 20.0
    self_conditioning: True
  atom_encoder:
    _target_: src.models.components.transformer.AtomAttentionEncoder
    n_blocks: 3
    n_heads: 4
    has_coords: True
    c_token: 384
    c_atom: 128
    c_atompair: 16
    c_s: 384
    c_z: 128
    n_queries: 32
    n_keys: 128
  transformer:
    _target_: src.models.components.transformer.DiffusionTransformer
    n_blocks: 12
    n_heads: 8
    c_a: 384
    c_s: 384
    c_z: 128
  atom_decoder:
    _target_: src.models.components.transformer.AtomAttentionDecoder
    n_blocks: 3
    n_heads: 4
    c_token: 384
    c_atom: 128
    c_atompair: 16
  # blocks_per_ckpt: 1
loss:
  _target_: src.models.loss.AllLosses
  diffusion_lddt_loss_dense: True
  diffusion_sparse_loss_enable: True
ema_config:
  ema_decay: -1.0
  eval_ema_only": False  # whether wandb only tracking ema checkpoint metrics
  ema_mutable_param_keywords: [ "" ]
optimizer_config:
  lr: 0.0018
  lr_scheduler: "af3"
  warmup_steps: 10
  max_steps: 100000
  min_lr_ratio: 0.1
  decay_every_n_steps: 50000
  grad_clip_norm: 5
  # Optim - Adam
  adam:
    beta1: 0.9
    beta2: 0.98
    weight_decay: 1e-7
    lr: 0.0018
    use_adamw: False
  # Optim - LRScheduler
  af3_lr_scheduler:
    warmup_steps: 10
    decay_every_n_steps: 20000
    decay_factor: 0.90
    lr: 0.0018
diffusion_sample: 4

# default hparam for sampling s2011
inference:
  n_replica: 5
  replica_per_batch: 5
  self_conditioning: true
  output_dir: ${paths.output_dir}/samples

# compile model for faster training with pytorch 2.0
compile: false

