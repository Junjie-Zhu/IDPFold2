_target_: src.models.idpfold_multimer.IDPFoldMultimer

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 1e-4
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

net:
  _target_: src.models.components.diffusion_module.DiffusionModule
  use_fine_grained_checkpoint: True,
  sigma_data: 16.,
  c_token: 768,
  c_atom: 128,
  c_atompair: 16,
  c_z: 128,
  c_s: 384,
  c_s_inputs: 1280,
  initialization:
    "zero_init_condition_transition": False,
    "zero_init_atom_encoder_residual_linear": False,
    "he_normal_init_atom_encoder_small_mlp": False,
    "he_normal_init_atom_encoder_output": False,
    "glorot_init_self_attention": False,
    "zero_init_adaln": True,
    "zero_init_residual_condition_transition": False,
    "zero_init_dit_output": True,
    "zero_init_atom_decoder_linear": False,
  atom_encoder:
    "n_blocks": 3,
    "n_heads": 4,
  transformer:
    "n_blocks": 24,
    "n_heads": 16,
  atom_decoder:
    "n_blocks": 3,
    "n_heads": 4,
  blocks_per_ckpt: 1

# default hparam for sampling s2011
inference:
  n_replica: 5
  replica_per_batch: 5
  self_conditioning: true
  output_dir: ${paths.output_dir}/samples

# compile model for faster training with pytorch 2.0
compile: false
