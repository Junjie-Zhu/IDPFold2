_target_: src.models.idpfold_multimer.IDPFoldMultimer

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 1e-4
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 5

net:
  _target_: src.models.components.diffusion_module.DiffusionModule
  use_fine_grained_checkpoint: False
  sigma_data: 16.
  c_token: 384
  c_atom: 128
  c_atompair: 16
  c_z: 128
  c_s: 384
  c_s_inputs: 1280
  initialization:
    zero_init_condition_transition: False
    zero_init_atom_encoder_residual_linear: False
    he_normal_init_atom_encoder_small_mlp: False
    he_normal_init_atom_encoder_output: False
    glorot_init_self_attention: False
    zero_init_adaln: True
    zero_init_residual_condition_transition: False
    zero_init_dit_output: True
    zero_init_atom_decoder_linear: False
  embedding_module:
    _target_: src.models.components.diffusion_module.EmbeddingModule
    init_embed_size: 256
    node_embed_size: 384
    edge_embed_size: 128
    embedding_size: 1280
    sigma_data: 16.0
    num_bins: 22
    min_bin: 1e-5
    max_bin: 20.0
    self_conditioning: True
  atom_encoder:
    _target_: src.models.components.transformer.AtomAttentionEncoder
    n_blocks: 3
    n_heads: 4
    has_coords: True
    c_token: 384
    c_atom: 128
    c_atompair: 16
    c_s: 384
    c_z: 128
    n_queries: 32
    n_keys: 128
  transformer:
    _target_: src.models.components.transformer.DiffusionTransformer
    n_blocks: 12
    n_heads: 8
    c_a: 384
    c_s: 384
    c_z: 128
  atom_decoder:
    _target_: src.models.components.transformer.AtomAttentionDecoder
    n_blocks: 3
    n_heads: 4
    c_token: 384
    c_atom: 128
    c_atompair: 16
  # blocks_per_ckpt: 1
loss:
  _target_: src.models.loss.AllLosses
  diffusion_lddt_loss_dense: True
  diffusion_sparse_loss_enable: True
ema_config:
  ema_decay: -1.0
  eval_ema_only": False  # whether wandb only tracking ema checkpoint metrics
  ema_mutable_param_keywords: [ "" ]
diffusion_sample: 4

# default hparam for sampling s2011
inference:
  n_replica: 5
  replica_per_batch: 5
  self_conditioning: true
  output_dir: ${paths.output_dir}/samples

# compile model for faster training with pytorch 2.0
compile: false

